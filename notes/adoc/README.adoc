
:toc:
:imagesdir: ../../img/

== Instructor Notes

The whole point of this small class is to lead to other discussions. I will
explore some of those discussions here like, how to scale an elastic cluster. I
will leave little pieces of information to help facilitate discussion.

*Question: When has someone been logging into our Windows machine?* 

* Why is that important? 
* Who or What IP is logging in? 
* Where are they Logging in from?

=== Primer on Windows Security Events with Winlogbeat

==== Prerequisites & Assumptions

* Configured AWS account at https://aws.amazon.com/premiumsupport/knowledge-center/create-and-activate-aws-account/
* Administrator Access to both machines
* ssh or putty (windows)
* RDP
* Git Installed
* Code Editor
* Comfort with Windows and Linux Command Line
* Some Familiarity with Elastic Components
* vi Text Editor

==== EC2 Components

The recommended setup is done before class. Every student will have a Windows
server and a Linux Elastic Stack to send their logs.

==== Setup Linux

===== Setup Linux and Windows EC2 Instances

Login to AWS https://console.aws.amazon.com. Search for the `All Services` and
find the `Compute` heading. From there select `EC2` 
image:snapshot1.png[image]

On the left pane, select `Instances` and then `Launch Instance`.
image:snapshot3.png[image]

Select the `Ubuntu Server 16.04 LTS (HVM), SSD Volume Type` for our Elastic
stack and `Microsoft Windows Server 2019 Base` for our Windows Machine.
image:snapshot4.png[image]

Select `t2.medium` instance type for both
instances, then select the `Next: Configure Instance Details` button.
image:snapshot5.png[image]

Unless anything special needs to be done, accept the defaults and select `Next: Add Storage`
image:snapshot6.png[image] 

A couple of factors will determine how much storage is required. I have found
250 GB is sufficient for a single node setup that has all the Elastic
components. As nodes are added, then the hard disks could be smaller. once you have decided the size then select `Next: Add Tags`
image:snapshot7.png[image]

If you have several students adding tags may be useful to assist with
troubleshooting and organization. Once completed select
`Next: Configure Security Group` 
image:snapshot8.png[image]

For the very first, instance you will set the security group. This security
group can be reused for the subsequent instances.

====== Windows

[cols=",,,,",options="header",]
|===
|Type |Protocol |Port Range |Source |Description
|RPD |TCP |3389 |0.0.0.0/0 |RDP Access
|===

====== Linux

[cols=",,,,",options="header",]
|===
|Type |Protocol |Port Range |Source |Description
|SSH |TCP |22 |0.0.0.0/0 |SSH Access
|Custom TCP Rule |TCP |5044 |172.31.32.0/20 |Beats
|Custom TCP Rule |TCP |5601 |0.0.0.0/0 |Kibana
|All ICMP - IPv4 |All |N/A |0.0.0.0/0 |Ping
|===

____
Why is the addressing setup this way? SSH and Kibana are set to everywhere
because we dont know what ip the student is comming from. ICMP facilitates ping.
Port 5044 is for winlogbeat. We set a specific IP range as we want to limit it
to just us.
____

image:snapshot9.png[image]

Review the details of the instance and select `Launch` when you satisfied with
the results


NOTE: Repeat as required for each student.


===== Install Elastic Components Via APT on Linux EC2 Instance

This is the recommended start of the class. It’s enough for the student to get
their ``hands dirty'' and lead to other discussions. My recommendation is to use
a single key for all the instances in the class. For each class the key needs to
be changed.

Log into the linux instance via ssh: 

* Linux users can use
`ssh -i "PathtoCert.pem" ubuntu@some-provided-address.compute-1.amazonaws.com`
* Windows users can follow the instructions here for putty https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/putty.html

Installing via rpm instead of an archive is a matter of preference here. In
order to simplify things a little, I am using apt to load the required packages.
To start the process we must add elastic keys with
`wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -`

Amazon EC2 instances should already have the `apt-transport-https` package
installed but just in case it does not run
`sudo apt-get install apt-transport-https`.

An entry to the sources list for ubuntu needs to be added. This can be done with
`echo "deb https://artifacts.elastic.co/packages/7.x/apt stable main" | sudo tee -a /etc/apt/sources.list.d/elastic-7.x.list`.

We need to update and install the Java environment with
`sudo apt-get update && sudo apt-get install default-jre`.


NOTE: Using `default-jre` _could_ install an unsupported version of Java. Make
sure it installs a supported version of Java by visiting
https://www.elastic.co/support/matrix#matrix_jvm.


Now that everything is setup we can actually install something. Enter
`sudo apt-get install elasticsearch logstash kibana` to install the Elastic
components.

===== Logstash

Starting with Logstash, we are going to create the Logstash configuration file.

Logstash it an *optional* component. It is not required in this scenario.
However, there will come a time when you want add capabilities. If you have
Logstash already running then reconfiguration will be much easier.

`sudo vi /etc/logstash/conf.d/logstash.conf`

This file defines how Logstash will input and output information.

Starting with the input we have set this up to receive a beats connection.
Logstash opens a port on 5044 to receive information for the beats clients. In our case, it will be only one but it could be an entire organization of windows machines if needed.

....
input {
  beats {
    port => 5044
  }
}
....

Next we have the output section. Since the elastic node is all-in-one, we
specify `localhost`. If we wanted to add additional Elasticsearch nodes this
would be a good place to start.

....
output {
  elasticsearch {
    hosts => ["http://localhost:9200"]
    index => "%{[@metadata][beat]}-%{[@metadata][version]}"
  }
}
....

* Example of our config file:

....
input {
  beats {
    port => 5044
  }
}

output {
  elasticsearch {
    hosts => ["http://localhost:9200"]
    index => "%{[@metadata][beat]}-%{[@metadata][version]}"
  }
}
....

If there were any special things that needed to be done to Logstash it would be performed in `/etc/logstash/logstash.yml`. This can be accomplished by
`sudo vi /etc/logstash/logstash.yml` As we have no changes, we will leave this alone.

Start Logstash`sudo systemctl start logstash`

Check Status of logstash `sudo systemctl status logstash`

===== Elasticsearch

You may have to elevate privileges with `sudo -s` to edit the configuration file for Elasticsearch `sudo vi /etc/elasticsearch/elasticsearch.yml`. Like
everything else with Elastic Stack there are a lot of things we can tweak. For simplicity, we are only going to change a few things.

Similar to Logstash, the default configuration will work. Once you start to have more nodes then there are some options that you will want to take into account.
Setting up cluster information can be done here. This cluster name will be the same across all configuration files.

[source,yml]
----
---------------------------------- Cluster -----------------------------------
#
# Use a descriptive name for your cluster:
#
cluster.name: my-application
#
----

Below is the section that includes the name for the node. When setting up a
cluster each node must have a unique name.


NOTE: System hostname and node name need to match

[source,yml]
----
# ------------------------------------ Node ------------------------------------
#
# Use a descriptive name for the node:
#
node.name: node-1
#
----

In the network section you can set ip that Elasticsearch will bind too.

[source,yml]
----
# ---------------------------------- Network -----------------------------------
#
# Set the bind address to a specific IP (IPv4 or IPv6):
#
network.host: 0.0.0.0
#
# Set a custom port for HTTP:
#
#http.port: 9200
#
# For more information, consult the network module documentation.
#
----

As more nodes are added, you will need to open ports 9200 (REST API) and 9300 (Node Communication). As we have everything one machine and only a single instance of Elasticsearch, we do not need to accomplish this. Once we reach the point we need additional nodes, the section below can be used as an example.

[source,yml]
----
#
# --------------------------------- Discovery ----------------------------------
#
# Pass an initial list of hosts to perform discovery when this node is started:
# The default list of hosts is ["127.0.0.1", "[::1]"]
#
discovery.seed_hosts: ["18.214.204.121"]
#
# Bootstrap the cluster using an initial set of master-eligible nodes:
#
#cluster.initial_master_nodes: ["node-1", "node-2"]
#
# For more information, consult the discovery and cluster formation module documentation.
#
----



Now that we have Elasticsearch installed and configured, lets go ahead and start Elasticsearch, `sudo systemctl start elasticsearch`.

===== Kibana

Last but not least on our AWS instance is Kibana. Just like the rest there is a configuration file. To edit the configuration file, use `sudo vi /etc/kibana/kibana.yml`. This machine does not reside on the same network as the students. When building we have two options here. - We can bind it to a specific IP address - 0.0.0.0.

For production binding it to a specific address is best. These instances are temporary so we do not need to worry about what IP it decides to use. This can be accomplished by changing the `server.host:` to `"0.0.0.0"` instead of localhost.


Save the file and Start kibana with `sudo systemctl start kibana`


NOTE: Ensure the Elastic services start without issues as Windows section requires that everything is functioning properly.


===== Finishing Up

To ensure everything is up and running, enter `sudo systemctl status logstash elasticsearch kibana`

....
● logstash.service - logstash
   Loaded: loaded (/etc/systemd/system/logstash.service; disabled; vendor preset: enabled)
   Active: active (running) since Sat 2020-02-01 15:16:04 UTC; 5s ago
 Main PID: 18797 (java)
    Tasks: 14
   Memory: 288.9M
      CPU: 6.955s
   CGroup: /system.slice/logstash.service
           └─18797 /usr/bin/java -Xms1g -Xmx1g -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -Djava.awt.he

Feb 01 15:16:04 ip-172-31-92-226 systemd[1]: Started logstash.

● elasticsearch.service - Elasticsearch
   Loaded: loaded (/usr/lib/systemd/system/elasticsearch.service; disabled; vendor preset: enabled)
   Active: active (running) since Thu 2020-01-30 20:04:32 UTC; 1 day 19h ago
     Docs: http://www.elastic.co
 Main PID: 14245 (java)
    Tasks: 60
   Memory: 1.3G
      CPU: 6min 10.151s
   CGroup: /system.slice/elasticsearch.service
           ├─14245 /usr/share/elasticsearch/jdk/bin/java -Des.networkaddress.cache.ttl=60 -Des.networkaddress.cache.negative.ttl=10 -XX:+AlwaysPreTouch -Xss1
           └─14339 /usr/share/elasticsearch/modules/x-pack-ml/platform/linux-x86_64/bin/controller

Jan 30 20:04:17 ip-172-31-92-226 systemd[1]: Starting Elasticsearch...
Jan 30 20:04:18 ip-172-31-92-226 elasticsearch[14245]: OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will lik
Jan 30 20:04:32 ip-172-31-92-226 systemd[1]: Started Elasticsearch.

● kibana.service - Kibana
   Loaded: loaded (/etc/systemd/system/kibana.service; disabled; vendor preset: enabled)
   Active: active (running) since Sat 2020-02-01 15:16:04 UTC; 5s ago
 Main PID: 18802 (node)
    Tasks: 11
   Memory: 279.0M
      CPU: 4.017s
   CGroup: /system.slice/kibana.service
           └─18802 /usr/share/kibana/bin/../node/bin/node /usr/share/kibana/bin/../src/cli -c /etc/kibana/kibana.yml

Feb 01 15:16:10 ip-172-31-92-226 kibana[18802]: {"type":"log","@timestamp":"2020-02-01T15:16:10Z","tags":["info","plugins","security"],"pid":18802,"message":
....

==== Discussion about Nodes
We are going to do everything on one node. However, can take a small trek through what it will take to scale to a cluster. This most likely candidate for clustering is Elasticsearch and then Logstash.

* For Elasticsearh, visit
https://www.elastic.co/guide/en/elasticsearch/reference/current/add-elasticsearch-nodes.html.
* For Logstash, visit
https://www.elastic.co/guide/en/logstash/current/deploying-and-scaling.html

There are lots of "It Depends…" here but as this is aimed at a beginner audience the main points are:

* Availability (Shards are distributed across all the nodes, protecting the data) 
* Throughput (Reads and writes will not overwhelm a cluster) 
* Distribution of effort (Shifting to dedicated node types: master nodes, ingest, etc.)

=== Setup Windows EC2
Select your Windows EC2 instance and select `Connect`. That will open a small
window. Click on `Get Password` to start the password retrieval process. It will ask you for the key in order to decrypt the password for the instance. Select `Elasticsearch.pem` and then `Decrypt Password`. You will be presented with the password to your instance. Use the information provided to access the windows EC2 instance.

NOTE: It can take up to 4 minutes after the Windows Instances starts to retrieve the password.

Once you have a remote desktop connection to your AWS instance. Download the
latest 64-bit version Winlogbeat zip file from the downloads page, in our case its 7.5.2.
`https://artifacts.elastic.co/downloads/beats/winlogbeat/winlogbeat-7.5.2-windows-x86_64.zip`

From your downloads folder, extract the zip file into `C:\Program Files` folder.
Rename the extracted file with the name `winlogbeat-<some_version>` folder to `Winlogbeat`. Open a PowerShell prompt as an *Administrator*. From start menu, right-click on the PowerShell icon and select `Run As Administrator`. To ensure that the script runs without issue we will execute the install of the service under a different execution policy via `PowerShell.exe -ExecutionPolicy UnRestricted -File .\install-service-winlogbeat.ps1`

Edit the config file for Winlogbeat notepad with `notepad C:\Program Files\Winlogbeat\winlogbeat.yml` We only need to edit a small section that pertains to Logstash and Elasticsearch. This will send the logs from our Windows server to our Linux server.

....
 #================================ Outputs =====================================

  # Configure what output to use when sending the data collected by the beat.

  #-------------------------- Elasticsearch output ------------------------------
  #output.elasticsearch:
  # Array of hosts to connect to.
  #  hosts: ["localhost:9200"]

  # Optional protocol and basic auth credentials.
  #protocol: "https"
  #username: "elastic"
  #password: "changeme"

  #----------------------------- Logstash output --------------------------------
  output.logstash:
  # The Logstash hosts
  hosts: ["18.214.204.121:5044"]
....

Since we are sending out data to Logstash instead of Elasticsearch then we need to manually setup the index template.

....
.\winlogbeat.exe setup --index-management -E output.logstash.enabled=false -E 'output.elasticsearch.hosts=["<<<IPofLogstashNode>>>:9200"]'
....

To setup dashboards when the Logstash output is enabled, you need to temporarily disable the Logstash output and enable Elasticsearch. To connect to a secured Elasticsearch cluster, you also need to pass Elasticsearch credentials. In our case it is not. In production it should be.

....
.\winlogbeat.exe setup -e `
  -E output.logstash.enabled=false `
  -E output.elasticsearch.hosts=['<<<IPofElasticsearchNode>>>:9200'] `
  -E setup.kibana.host=<<<IPofKibanaNode>>>:5601
....

Test the winlogbeat config
`C:\Program Files\Winlogbeat> .\winlogbeat.exe test config -c .\winlogbeat.yml -e`. You can now run `services.msc` to start it.

=== Answering the Question

And now the fun part! Answering the questions! Navigate to Kibana IP `<<ip_of_linux>>:5601`. Pivot to the `Dashboards` section on the left pane. Look for event ID `4624` That is the event id for a windows login.

=== Questions ?

*Question: When has someone been logging into our Windows machine?* 

* Why is that important? 
* Who or What IP is logging in? 
* Where are they Logging in from?